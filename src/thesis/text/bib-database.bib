@article{Khurana2023,
  author = {Diksha Khurana and Aditya Koli and Kiran Khatter and Sukhdev Singh},
  journal = {Multimedia Tools and Applications},
  title = {Natural language processing: state of the art, current trends and challenges},
  year = {2023},
  volume = {82},
  number = {3},
  ISSN = {1573-7721},
  pages = {3713--3744},
  medium = {online},
  accessed = {2025-03-26},
  DOI = {10.1007/s11042-022-13428-4},
  URL = {https://doi.org/10.1007/s11042-022-13428-4},
}

@online{w3techs-stats,
  author    = {W3Techs},
  title     = {Usage statistics of content languages for websites},
  year      = {2025},
  url       = {https://w3techs.com/technologies/overview/content_language},
  urldate   = {2025-03-28},
}

@online{ujc-prirucka,
  author    = {√östav pro jazyk ƒçesk√Ω AV ƒåR, v.~v.~i.},
  title     = {Internetov√° jazykov√° p≈ô√≠ruƒçka},
  note      = {Morfematika},
  url       = {https://prirucka.ujc.cas.cz/?id=778},
  urldate   = {2025-03-29},
}

@Inbook{Hiemstra2009,
author="Hiemstra, Djoerd",
editor="LIU, LING
and {\"O}ZSU, M. TAMER",
title="Language Models",
bookTitle="Encyclopedia of Database Systems",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="1591--1594",
isbn="978-0-387-39940-9",
doi="10.1007/978-0-387-39940-9_923",
url="https://doi.org/10.1007/978-0-387-39940-9_923"
}

@article{Wei_2024,
   title={An Overview of Language Models: Recent Developments and Outlook},
   volume={13},
   ISSN={2048-7703},
   url={http://dx.doi.org/10.1561/116.00000010},
   DOI={10.1561/116.00000010},
   number={2},
   journal={APSIPA Transactions on Signal and Information Processing},
   publisher={Now Publishers},
   author={Wei, Chengwei and Wang, Yun-Cheng and Wang, Bin and Kuo, C.-C. Jay},
   year={2024} 
}

@misc{zhou2023comprehensivesurveypretrainedfoundation,
      title={A Comprehensive Survey on Pretrained Foundation Models: A History from BERT to ChatGPT}, 
      author={Ce Zhou and Qian Li and Chen Li and Jun Yu and Yixin Liu and Guangjing Wang and Kai Zhang and Cheng Ji and Qiben Yan and Lifang He and Hao Peng and Jianxin Li and Jia Wu and Ziwei Liu and Pengtao Xie and Caiming Xiong and Jian Pei and Philip S. Yu and Lichao Sun},
      year={2023},
      eprint={2302.09419},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://doi.org/10.48550/arXiv.2302.09419}, 
      doi={10.48550/arXiv.2302.09419}
}

@article{brown-etal-1992-class,
    title = "Class-Based \textit{n}-gram Models of Natural Language",
    author = "Brown, Peter F.  and
      Della Pietra, Vincent J.  and
      deSouza, Peter V.  and
      Lai, Jenifer C.  and
      Mercer, Robert L.",
    journal = "Computational Linguistics",
    volume = "18",
    number = "4",
    year = "1992",
    url = "https://aclanthology.org/J92-4003/",
    pages = "467--480"
}

@article{radford2018improving,
  title={Improving language understanding by generative pre-training},
  author={Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya and others},
  year={2018},
  publisher={San Francisco, CA, USA},
  url={https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf}
}

@misc{devlin2019bert,
      title={BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding}, 
      author={Jacob Devlin and Ming-Wei Chang and Kenton Lee and Kristina Toutanova},
      year={2019},
      eprint={1810.04805},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1810.04805}, 
}

@online{huggingface,
  author    = {HuggingFace},
  title     = {The AI community building the future.},
  year      = {2025},
  url       = {https://huggingface.co},
  urldate   = {2025-04-01},
}

@misc{min2021recentadvancesnaturallanguage,
      title={Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey}, 
      author={Bonan Min and Hayley Ross and Elior Sulem and Amir Pouran Ben Veyseh and Thien Huu Nguyen and Oscar Sainz and Eneko Agirre and Ilana Heinz and Dan Roth},
      year={2021},
      eprint={2111.01243},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.01243}, 
}

@article{Elazar2021,
    author = {Elazar, Yanai and Kassner, Nora and Ravfogel, Shauli and Ravichander, Abhilasha and Hovy, Eduard and Sch√ºtze, Hinrich and Goldberg, Yoav},
    title = {Measuring and Improving Consistency in Pretrained Language Models},
    journal = {Transactions of the Association for Computational Linguistics},
    volume = {9},
    pages = {1012-1031},
    year = {2021},
    month = {12},
    abstract = {Consistency of a model‚Äîthat is, the invariance of its behavior under meaning-preserving alternations in its input‚Äîis a highly desirable property in natural language processing. In this paper we study the question: Are Pretrained Language Models (PLMs) consistent with respect to factual knowledge? To this end, we create ParaRelü§ò, a high-quality resource of cloze-style query English paraphrases. It contains a total of 328 paraphrases for 38 relations. Using ParaRelü§ò, we show that the consistency of all PLMs we experiment with is poor‚Äî though with high variance between relations. Our analysis of the representational spaces of PLMs suggests that they have a poor structure and are currently not suitable for representing knowledge robustly. Finally, we propose a method for improving model consistency and experimentally demonstrate its effectiveness.1},
    issn = {2307-387X},
    doi = {10.1162/tacl_a_00410},
    url = {https://doi.org/10.1162/tacl\_a\_00410},
    eprint = {https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl\_a\_00410/1975957/tacl\_a\_00410.pdf},
}

@misc{vaswani2023attentionneed,
      title={Attention Is All You Need}, 
      author={Ashish Vaswani and Noam Shazeer and Niki Parmar and Jakob Uszkoreit and Llion Jones and Aidan N. Gomez and Lukasz Kaiser and Illia Polosukhin},
      year={2023},
      eprint={1706.03762},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1706.03762}, 
}

@article{WANG202351,
title = {Pre-Trained Language Models and Their Applications},
journal = {Engineering},
volume = {25},
pages = {51-65},
year = {2023},
issn = {2095-8099},
doi = {https://doi.org/10.1016/j.eng.2022.04.024},
url = {https://www.sciencedirect.com/science/article/pii/S2095809922006324},
author = {Haifeng Wang and Jiwei Li and Hua Wu and Eduard Hovy and Yu Sun},
keywords = {Pre-trained models, Natural language processing},
abstract = {Pre-trained language models have achieved striking success in natural language processing (NLP), leading to a paradigm shift from supervised learning to pre-training followed by fine-tuning. The NLP community has witnessed a surge of research interest in improving pre-trained models. This article presents a comprehensive review of representative work and recent progress in the NLP field and introduces the taxonomy of pre-trained models. We first give a brief introduction of pre-trained models, followed by characteristic methods and frameworks. We then introduce and analyze the impact and challenges of pre-trained models and their downstream applications. Finally, we briefly conclude and address future research directions in this field.}
}

@inproceedings{pfeiffer-etal-2021-adapterfusion,
    title = "{A}dapter{F}usion: Non-Destructive Task Composition for Transfer Learning",
    author = {Pfeiffer, Jonas  and
      Kamath, Aishwarya  and
      R{\"u}ckl{\'e}, Andreas  and
      Cho, Kyunghyun  and
      Gurevych, Iryna},
    editor = "Merlo, Paola  and
      Tiedemann, Jorg  and
      Tsarfaty, Reut",
    booktitle = "Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume",
    month = apr,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.eacl-main.39/",
    doi = "10.18653/v1/2021.eacl-main.39",
    pages = "487--503",
    abstract = "Sequential fine-tuning and multi-task learning are methods aiming to incorporate knowledge from multiple tasks; however, they suffer from catastrophic forgetting and difficulties in dataset balancing. To address these shortcomings, we propose AdapterFusion, a new two stage learning algorithm that leverages knowledge from multiple tasks. First, in the knowledge extraction stage we learn task specific parameters called adapters, that encapsulate the task-specific information. We then combine the adapters in a separate knowledge composition step. We show that by separating the two stages, i.e., knowledge extraction and knowledge composition, the classifier can effectively exploit the representations learned from multiple tasks in a non-destructive manner. We empirically evaluate AdapterFusion on 16 diverse NLU tasks, and find that it effectively combines various types of knowledge at different layers of the model. We show that our approach outperforms traditional strategies such as full fine-tuning as well as multi-task learning. Our code and adapters are available at AdapterHub.ml."
}

@misc{houlsby2019parameterefficienttransferlearningnlp,
      title={Parameter-Efficient Transfer Learning for NLP}, 
      author={Neil Houlsby and Andrei Giurgiu and Stanislaw Jastrzebski and Bruna Morrone and Quentin de Laroussilhe and Andrea Gesmundo and Mona Attariyan and Sylvain Gelly},
      year={2019},
      eprint={1902.00751},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/1902.00751}, 
}

@misc{mielke2021wordscharactersbriefhistory,
      title={Between words and characters: A Brief History of Open-Vocabulary Modeling and Tokenization in NLP}, 
      author={Sabrina J. Mielke and Zaid Alyafeai and Elizabeth Salesky and Colin Raffel and Manan Dey and Matthias Gall√© and Arun Raja and Chenglei Si and Wilson Y. Lee and Beno√Æt Sagot and Samson Tan},
      year={2021},
      eprint={2112.10508},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.10508}, 
}

@inproceedings{webster-kit-1992-tokenization,
    title = "Tokenization as the Initial Phase in {NLP}",
    author = "Webster, Jonathan J.  and
      Kit, Chunyu",
    booktitle = "{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics",
    year = "1992",
    url = "https://aclanthology.org/C92-4173/"
}

@online{tokenizacemultilingual,
  author    = {Yennie Jun},
  title     = {All languages are NOT created (tokenized) equal},
  year      = {2023},
  url       = {https://web.archive.org/web/20230817165705/https://blog.yenniejun.com/p/all-languages-are-not-created-tokenized#expand},
  urldate   = {2025-04-04},
}

@misc{brown2020languagemodelsfewshotlearners,
      title={Language Models are Few-Shot Learners}, 
      author={Tom B. Brown and Benjamin Mann and Nick Ryder and Melanie Subbiah and Jared Kaplan and Prafulla Dhariwal and Arvind Neelakantan and Pranav Shyam and Girish Sastry and Amanda Askell and Sandhini Agarwal and Ariel Herbert-Voss and Gretchen Krueger and Tom Henighan and Rewon Child and Aditya Ramesh and Daniel M. Ziegler and Jeffrey Wu and Clemens Winter and Christopher Hesse and Mark Chen and Eric Sigler and Mateusz Litwin and Scott Gray and Benjamin Chess and Jack Clark and Christopher Berner and Sam McCandlish and Alec Radford and Ilya Sutskever and Dario Amodei},
      year={2020},
      eprint={2005.14165},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2005.14165}, 
}

@misc{minaee2025largelanguagemodelssurvey,
      title={Large Language Models: A Survey}, 
      author={Shervin Minaee and Tomas Mikolov and Narjes Nikzad and Meysam Chenaghlu and Richard Socher and Xavier Amatriain and Jianfeng Gao},
      year={2025},
      eprint={2402.06196},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2402.06196}, 
}

@misc{naveed2024comprehensiveoverviewlargelanguage,
      title={A Comprehensive Overview of Large Language Models}, 
      author={Humza Naveed and Asad Ullah Khan and Shi Qiu and Muhammad Saqib and Saeed Anwar and Muhammad Usman and Naveed Akhtar and Nick Barnes and Ajmal Mian},
      year={2024},
      eprint={2307.06435},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2307.06435}, 
}

@online{modernbertreplacment,
  author    = {Benjamin Warner and Antoine Chaffin and Benjamin Clavi√© and Orion Weller and Oskar Hallstr√∂m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Jonathan Whitaker and Iacopo Poli},
  title     = {Finally, a Replacement for BERT},
  year      = {2024},
  url       = {https://huggingface.co/blog/modernbert},
  urldate   = {2025-04-04},
}

@misc{liu2019robertarobustlyoptimizedbert,
      title={RoBERTa: A Robustly Optimized BERT Pretraining Approach}, 
      author={Yinhan Liu and Myle Ott and Naman Goyal and Jingfei Du and Mandar Joshi and Danqi Chen and Omer Levy and Mike Lewis and Luke Zettlemoyer and Veselin Stoyanov},
      year={2019},
      eprint={1907.11692},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1907.11692}, 
}

@misc{warner2024smarterbetterfasterlonger,
      title={Smarter, Better, Faster, Longer: A Modern Bidirectional Encoder for Fast, Memory Efficient, and Long Context Finetuning and Inference}, 
      author={Benjamin Warner and Antoine Chaffin and Benjamin Clavi√© and Orion Weller and Oskar Hallstr√∂m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
      year={2024},
      eprint={2412.13663},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2412.13663}, 
}


@article{MAO2024102048,
title = {Sentiment analysis methods, applications, and challenges: A systematic literature review},
journal = {Journal of King Saud University - Computer and Information Sciences},
volume = {36},
number = {4},
pages = {102048},
year = {2024},
issn = {1319-1578},
doi = {https://doi.org/10.1016/j.jksuci.2024.102048},
url = {https://www.sciencedirect.com/science/article/pii/S131915782400137X},
author = {Yanying Mao and Qun Liu and Yu Zhang},
keywords = {Sentiment analysis, Methods, Applications, Large language models, Challenges},
abstract = {With the expansion of Internet-based applications, the number of comments shows explosive growth. Analyzing the attitudes and emotions behind comments provides powerful assistance for businesses, governments, and scholars. However, it is hard to effectively extract user‚Äôs attitude from the massive amounts of comments. Sentiment analysis (SA) provides an automatic, fast and efficient tool to identify reviewers‚Äô opinions and sentiments. However, the existing literature reviews cover a limited number of studies or have a narrow field of studies for sentiment analysis. This paper provided a systematic literature review of sentiment analysis methods, applications, and challenges. This systematic literature review gives insights into the goal of the sentiment analysis task, offers comparisons of different techniques, investigates the application domains of sentiment analysis, highlights the challenges and limitations encountered by scholars, provides suggestions on possible solutions and explores the future research directions. The study‚Äôs findings emphasize the significant role of artificial intelligence technologies in automatic text sentiment analysis and highlight the importance of sentiment analysis in people‚Äôs production and life. This research not only contributes to the existing sentiment analysis knowledge body but also provides references to scholars and practitioners in choosing a suitable methodology and good practices to perform sentiment analysis.}
}

@inbook{Aqlanstudyofsentiment,
author = {Aqlan, Ameen and Bairam, Dr. Manjula and Naik, R Lakshman},
year = {2019},
month = {01},
pages = {147-162},
title = {A Study of Sentiment Analysis: Concepts, Techniques, and Challenges},
isbn = {978-1-4939-9044-3},
doi = {10.1007/978-981-13-6459-4_16}
}

@article{SuryawanshiSA,
author = {Suryawanshi, Nikhil},
year = {2024},
month = {06},
pages = {005-015},
title = {Sentiment analysis with machine learning and deep learning: A survey of techniques and applications},
volume = {12},
journal = {International Journal of Science and Research Archive},
doi = {10.30574/ijsra.2024.12.2.1205}
}

@misc{zhang2018deeplearningsentimentanalysis,
      title={Deep Learning for Sentiment Analysis : A Survey}, 
      author={Lei Zhang and Shuai Wang and Bing Liu},
      year={2018},
      eprint={1801.07883},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1801.07883}, 
}

@misc{kumar2023comprehensivereviewsentimentanalysis,
      title={A Comprehensive Review on Sentiment Analysis: Tasks, Approaches and Applications}, 
      author={Sudhanshu Kumar and Partha Pratim Roy and Debi Prosad Dogra and Byung-Gyu Kim},
      year={2023},
      eprint={2311.11250},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2311.11250}, 
}

@inproceedings{NasukawaSA,
author = {Nasukawa, Tetsuya and Yi, Jeonghee},
title = {Sentiment analysis: capturing favorability using natural language processing},
year = {2003},
isbn = {1581135831},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/945645.945658},
doi = {10.1145/945645.945658},
abstract = {This paper illustrates a sentiment analysis approach to extract sentiments associated with polarities of positive or negative for specific subjects from a document, instead of classifying the whole document into positive or negative.The essential issues in sentiment analysis are to identify how sentiments are expressed in texts and whether the expressions indicate positive (favorable) or negative (unfavorable) opinions toward the subject. In order to improve the accuracy of the sentiment analysis, it is important to properly identify the semantic relationships between the sentiment expressions and the subject. By applying semantic analysis with a syntactic parser and sentiment lexicon, our prototype system achieved high precision (75-95\%, depending on the data) in finding sentiments within Web pages and news articles.},
booktitle = {Proceedings of the 2nd International Conference on Knowledge Capture},
pages = {70‚Äì77},
numpages = {8},
keywords = {favorability analysis, information extraction, sentiment analysis, text mining},
location = {Sanibel Island, FL, USA},
series = {K-CAP '03}
}

@article{XU2020135,
title = {Aspect-based sentiment classification with multi-attention network},
journal = {Neurocomputing},
volume = {388},
pages = {135-143},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2020.01.024},
url = {https://www.sciencedirect.com/science/article/pii/S092523122030059X},
author = {Qiannan Xu and Li Zhu and Tao Dai and Chengbing Yan},
keywords = {Aspect-based sentiment classification, Sentiment analysis, Attention mechanism, Neural network},
abstract = {Aspect-based sentiment classification aims to predict the sentiment polarity of an aspect term in a sentence instead of the sentiment polarity of the entire sentence. Neural networks have been used for this task, and most existing methods have adopted sequence models, which require more training time than other models. When an aspect term comprises several words, most methods involve a coarse-level attention mechanism to model the aspect, and this may result in information loss. In this paper, we propose a multi-attention network (MAN) to address the above problems. The proposed model uses intra- and inter-level attention mechanisms. In the former, the MAN employs a transformer encoder instead of a sequence model to reduce training time. The transformer encoder encodes the input sentence in parallel and preserves long-distance sentiment relations. In the latter, the MAN uses a global and a local attention module to capture differently grained interactive information between aspect and context. The global attention module focuses on the entire relation, whereas the local attention module considers interactions at word level; this was often neglected in previous studies. Experiments demonstrate that the proposed model achieves superior results when compared to the baseline models.}
}

@article{TAN2020336,
title = {Improving aspect-based sentiment analysis via aligning aspect embedding},
journal = {Neurocomputing},
volume = {383},
pages = {336-347},
year = {2020},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2019.12.035},
url = {https://www.sciencedirect.com/science/article/pii/S0925231219317382},
author = {Xingwei Tan and Yi Cai and Jingyun Xu and Ho-Fung Leung and Wenhao Chen and Qing Li},
keywords = {Aspect embedding, Sentiment analysis, Representation learning},
abstract = {Aspect-Based Sentiment Analysis (ABSA) is a fine-grained sentiment analysis task, which aims to predict sentiment polarities of given aspects or target terms in text. ABSA contains two subtasks: Aspect-Category Sentiment Analysis (ACSA) and Aspect-Term Sentiment Analysis (ATSA). Aspect embeddings have been extensively used for representing aspect-categories on ACSA task. Based on our observations, existing aspect embeddings cannot properly represent the relation between aspect-categories and aspect-terms. To address this limitation, this paper presents a learning method which trains aspect embeddings according to the relation between aspect-categories and aspect-terms. According to the cosine measure metric we proposed in this paper, the limitation is successfully alleviated in the aspect embeddings which are trained by our method. The trained aspect embeddings can be used as initialization in existing models to solve ACSA task. We conduct experiments on SemEval datasets for ACSA task, and the results indicate that our pre-trained aspect embeddings are capable of improving the performance of sentiment analysis.}
}

@misc{yan2021unifiedgenerativeframeworkaspectbased,
      title={A Unified Generative Framework for Aspect-Based Sentiment Analysis}, 
      author={Hang Yan and Junqi Dai and Tuo ji and Xipeng Qiu and Zheng Zhang},
      year={2021},
      eprint={2106.04300},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2106.04300}, 
}

@inproceedings{chebolu-etal-2024-oats,
    title = "{OATS}: A Challenge Dataset for Opinion Aspect Target Sentiment Joint Detection for Aspect-Based Sentiment Analysis",
    author = "Chebolu, Siva Uday Sampreeth  and
      Dernoncourt, Franck  and
      Lipka, Nedim  and
      Solorio, Thamar",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.1080/",
    pages = "12336--12347",
    abstract = "Aspect-based sentiment analysis (ABSA) delves into understanding sentiments specific to distinct elements within a user-generated review. It aims to analyze user-generated reviews to determine a) the target entity being reviewed, b) the high-level aspect to which it belongs, c) the sentiment words used to express the opinion, and d) the sentiment expressed toward the targets and the aspects. While various benchmark datasets have fostered advancements in ABSA, they often come with domain limitations and data granularity challenges. Addressing these, we introduce the OATS dataset, which encompasses three fresh domains and consists of 27,470 sentence-level quadruples and 17,092 review-level tuples. Our initiative seeks to bridge specific observed gaps in existing datasets: the recurrent focus on familiar domains like restaurants and laptops, limited data for intricate quadruple extraction tasks, and an occasional oversight of the synergy between sentence and review-level sentiments. Moreover, to elucidate OATS`s potential and shed light on various ABSA subtasks that OATS can solve, we conducted experiments, establishing initial baselines. We hope the OATS dataset augments current resources, paving the way for an encompassing exploration of ABSA (https://github.com/RiTUAL-UH/OATS-ABSA)."
}

@inproceedings{zhang-etal-2021-towards-generative,
    title = "Towards Generative Aspect-Based Sentiment Analysis",
    author = "Zhang, Wenxuan  and
      Li, Xin  and
      Deng, Yang  and
      Bing, Lidong  and
      Lam, Wai",
    editor = "Zong, Chengqing  and
      Xia, Fei  and
      Li, Wenjie  and
      Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-short.64/",
    doi = "10.18653/v1/2021.acl-short.64",
    pages = "504--510",
    abstract = "Aspect-based sentiment analysis (ABSA) has received increasing attention recently. Most existing work tackles ABSA in a discriminative manner, designing various task-specific classification networks for the prediction. Despite their effectiveness, these methods ignore the rich label semantics in ABSA problems and require extensive task-specific designs. In this paper, we propose to tackle various ABSA tasks in a unified generative framework. Two types of paradigms, namely annotation-style and extraction-style modeling, are designed to enable the training process by formulating each ABSA task as a text generation problem. We conduct experiments on four ABSA tasks across multiple benchmark datasets where our proposed generative approach achieves new state-of-the-art results in almost all cases. This also validates the strong generality of the proposed framework which can be easily adapted to arbitrary ABSA task without additional task-specific model design."
}

@misc{li2020multiinstancemultilabellearningnetworks,
      title={Multi-Instance Multi-Label Learning Networks for Aspect-Category Sentiment Analysis}, 
      author={Yuncong Li and Cunxiang Yin and Sheng-hua Zhong and Xu Pan},
      year={2020},
      eprint={2010.02656},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.02656}, 
}

@article{Peng_Xu_Bing_Huang_Lu_Si_2020, 
title={Knowing What, How and Why: A Near Complete Solution for Aspect-Based Sentiment Analysis}, 
volume={34}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/6383}, 
DOI={10.1609/aaai.v34i05.6383}, 
abstractNote={&lt;p&gt;Target-based sentiment analysis or aspect-based sentiment analysis (ABSA) refers to addressing various sentiment analysis tasks at a fine-grained level, which includes but is not limited to aspect extraction, aspect sentiment classification, and opinion extraction. There exist many solvers of the above individual subtasks or a combination of two subtasks, and they can work together to tell a complete story, i.e. the discussed aspect, the sentiment on it, and the cause of the sentiment. However, no previous ABSA research tried to provide a complete solution in one shot. In this paper, we introduce a new subtask under ABSA, named aspect sentiment triplet extraction (&lt;strong&gt;ASTE&lt;/strong&gt;). Particularly, a solver of this task needs to extract triplets (What, How, Why) from the inputs, which show WHAT the targeted aspects are, HOW their sentiment polarities are and WHY they have such polarities (i.e. opinion reasons). For instance, one triplet from ‚ÄúWaiters are very friendly and the pasta is simply average‚Äù could be (‚ÄòWaiters‚Äô, positive, ‚Äòfriendly‚Äô). We propose a two-stage framework to address this task. The first stage predicts what, how and why in a unified model, and then the second stage pairs up the predicted what (how) and why from the first stage to output triplets. In the experiments, our framework has set a benchmark performance in this novel triplet extraction task. Meanwhile, it outperforms a few strong baselines adapted from state-of-the-art related methods.&lt;/p&gt;}, 
number={05}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Peng, Haiyun and Xu, Lu and Bing, Lidong and Huang, Fei and Lu, Wei and Si, Luo}, 
year={2020},
month={4}, 
pages={8600-8607} }

@article{Wan_Yang_Du_Liu_Qi_Pan_2020, 
title={Target-Aspect-Sentiment Joint Detection for Aspect-Based Sentiment Analysis}, 
volume={34}, url={https://ojs.aaai.org/index.php/AAAI/article/view/6447}, 
DOI={10.1609/aaai.v34i05.6447}, 
abstractNote={&lt;p&gt;&lt;em&gt;Aspect-based sentiment analysis&lt;/em&gt; (ABSA) aims to detect the targets (which are composed by continuous words), aspects and sentiment polarities in text. Published datasets from SemEval-2015 and SemEval-2016 reveal that a sentiment polarity depends on both the target and the aspect. However, most of the existing methods consider predicting sentiment polarities from either targets or aspects but not from both, thus they easily make wrong predictions on sentiment polarities. In particular, where the target is implicit, &lt;em&gt;i.e.&lt;/em&gt;, it does not appear in the given text, the methods predicting sentiment polarities from targets do not work. To tackle these limitations in ABSA, this paper proposes a novel method for target-aspect-sentiment joint detection. It relies on a pre-trained language model and can capture the dependence on both targets and aspects for sentiment prediction. Experimental results on the SemEval-2015 and SemEval-2016 restaurant datasets show that the proposed method achieves a high performance in detecting target-aspect-sentiment triples even for the implicit target cases; moreover, it even outperforms the state-of-the-art methods for those subtasks of target-aspect-sentiment detection that they are competent to.&lt;/p&gt;}, 
number={05}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Wan, Hai and Yang, Yufei and Du, Jianfeng and Liu, Yanan and Qi, Kunxun and Pan, Jeff Z.}, 
year={2020}, 
month={4}, 
pages={9122-9129} }

@Article{Ke2023,
author={Ke, Cai and Xiong, Qingyu and Wu, Chao and Yi, Hualing and Gao, Min and Chen, Jie},
title={SimCPD: a simple framework for contrastive prompts of target-aspect-sentiment joint detection},
journal={Neural Computing and Applications},
year={2023},
month={8},
day={01},
volume={35},
number={22},
pages={16577-16592},
abstract={Target-aspect-sentiment detection (TASD) task is a vital task for aspect-based sentiment analysis, which aims to detect (target, aspect, sentiment) triples from sentences. However, most of the existing methods do not generate high-quality textual representations and thus produce poor performances on the TASD task. Moreover, how to deal with insufficient high-quality labeled data is also a great challenge. In this paper, we design DPDA, a Dual Prompts-based Data Augmentation method, that can generate diverse sentences with a question prompt view and an answer prompt view for each input. Then, we propose SimCPD, a Simple Framework for Contrastive Prompts of Target-Aspect-Sentiment Joint Detection, that maximizes the agreement between the two prompt views to obtain high-quality textual representations. Experiments on SemEval-2015 and SemEval-2016 demonstrate that the proposed DPDA can generate high-quality semantic information and alleviate the problem of sparsely labeled data. Moreover, the proposed SimCPD can detect (target, aspect, sentiment) triples more efficiently than other methods. Furthermore, it also achieves better performance on five TASD subtasks.},
issn={1433-3058},
doi={10.1007/s00521-023-08529-6},
url={https://doi.org/10.1007/s00521-023-08529-6}
}

@inproceedings{cai-etal-2021-aspect,
    title = "Aspect-Category-Opinion-Sentiment Quadruple Extraction with Implicit Aspects and Opinions",
    author = "Cai, Hongjie and Xia, Rui and Yu, Jianfei",
    editor = "Zong, Chengqing and Xia, Fei and Li, Wenjie and Navigli, Roberto",
    booktitle = "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = aug,
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.acl-long.29/",
    doi = "10.18653/v1/2021.acl-long.29",
    pages = "340--350",
    abstract = "Product reviews contain a large number of implicit aspects and implicit opinions. However, most of the existing studies in aspect-based sentiment analysis ignored this problem. In this work, we introduce a new task, named Aspect-Category-Opinion-Sentiment (ACOS) Quadruple Extraction, with the goal to extract all aspect-category-opinion-sentiment quadruples in a review sentence and provide full support for aspect-based sentiment analysis with implicit aspects and opinions. We furthermore construct two new datasets, Restaurant-ACOS and Laptop-ACOS, for this new task, both of which contain the annotations of not only aspect-category-opinion-sentiment quadruples but also implicit aspects and opinions. The former is an extension of the SemEval Restaurant dataset; the latter is a newly collected and annotated Laptop dataset, twice the size of the SemEval Laptop dataset. We finally benchmark the task with four baseline systems. Experiments demonstrate the feasibility of the new task and its effectiveness in extracting and describing implicit aspects and implicit opinions. The two datasets and source code of four systems are publicly released at \url{https://github.com/NUSTM/ACOS}."
}

@article{RNNbib,
author = {Elman, Jeffrey L.},
title = {Finding Structure in Time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179-211},
doi = {https://doi.org/10.1207/s15516709cog1402\_1},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
year = {1990}
}

@article{LSTMbib,
    author = {Hochreiter, Sepp and Schmidhuber, J√ºrgen},
    title = {Long Short-Term Memory},
    journal = {Neural Computation},
    volume = {9},
    number = {8},
    pages = {1735-1780},
    year = {1997},
    month = {11},
    abstract = {Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.},
    issn = {0899-7667},
    doi = {10.1162/neco.1997.9.8.1735},
    url = {https://doi.org/10.1162/neco.1997.9.8.1735},
    eprint = {https://direct.mit.edu/neco/article-pdf/9/8/1735/813796/neco.1997.9.8.1735.pdf},
}

@misc{bahdanau2016neuralmachinetranslationjointly,
      title={Neural Machine Translation by Jointly Learning to Align and Translate}, 
      author={Dzmitry Bahdanau and Kyunghyun Cho and Yoshua Bengio},
      year={2016},
      eprint={1409.0473},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1409.0473}, 
}

@misc{weston2015memorynetworks,
      title={Memory Networks}, 
      author={Jason Weston and Sumit Chopra and Antoine Bordes},
      year={2015},
      eprint={1410.3916},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/1410.3916}, 
}

@misc{sukhbaatar2015endtoendmemorynetworks,
      title={End-To-End Memory Networks}, 
      author={Sainbayar Sukhbaatar and Arthur Szlam and Jason Weston and Rob Fergus},
      year={2015},
      eprint={1503.08895},
      archivePrefix={arXiv},
      primaryClass={cs.NE},
      url={https://arxiv.org/abs/1503.08895}, 
}

@Article{Sreesurya2020,
author={Sreesurya, Ilayaraja and Rathi, Himani and Jain, Pooja and Jain, Tapan Kumar},
title={Hypex: A Tool for Extracting Business Intelligence from Sentiment Analysis using Enhanced LSTM},
journal={Multimedia Tools and Applications},
year={2020},
month={12},
day={01},
volume={79},
number={47},
pages={35641-35663},
abstract={Sentiment analysis, an application of machine learning in business is the process of identifying and cataloging comments, reviews, tweets, feedback, and even random rants according to the tone or sentiments conveyed by it. The data is analysed using machine learning approach of Long Short Term Memory (LSTM) rating the sentiments on a scale ranging from -100 to 100. A new proposed activation function is used for LSTM giving best results as compared to the existing Artificial Neural Network (ANN) techniques. Depending upon the mined opinion, the business intelligence tools evaluate the products or services of a company eventually resulting in the increase of the sales of that company. The results clearly show that BI extracted from SA is quite instrumental in driving business effectiveness and innovation.},
issn={1573-7721},
doi={10.1007/s11042-020-08930-6},
url={https://doi.org/10.1007/s11042-020-08930-6}
}

@article{KUMAR201941,
title = {Fusion of EEG response and sentiment analysis of products review to predict customer satisfaction},
journal = {Information Fusion},
volume = {52},
pages = {41-52},
year = {2019},
issn = {1566-2535},
doi = {https://doi.org/10.1016/j.inffus.2018.11.001},
url = {https://www.sciencedirect.com/science/article/pii/S1566253517304918},
author = {Sudhanshu Kumar and Mahendra Yadava and Partha Pratim Roy},
keywords = {Neuroscience, EEG, Sentiment analysis, Rating prediction, ABC, Multimodal},
abstract = {This paper proposes a novel multimodal framework for rating prediction of consumer products by fusing different data sources, namely physiological signals, global reviews obtained separately for the product and its brand. The reviews posted by global viewers are retrieved and processed using Natural Language Processing (NLP) technique to compute compound score considered as global rating. Also, electroencephalogram (EEG) signals of the participants were recorded simultaneously while watching different products on computer‚Äôs screen. From EEG, valence scores in terms of product rating are obtained using self-report towards each viewed product for acquiring local rating. A higher valence score corresponds to intrinsic attractiveness of the participant towards a product. Random forest based regression techniques is used to model EEG data to build a rating prediction framework considered as local rating. Furthermore, Artificial Bee Colony (ABC) based optimization algorithm is used to boost the overall performance of the framework by fusing global and local ratings. EEG dataset of 40 participants including 25 male and 15 female is recorded while viewing 42 different products available on e-commerce website. Experiment results are encouraging and suggest that the proposed ABC optimization approach can achieve lower Root Mean Square Error (RMSE) in rating prediction as compared to individual unimodal schemes.}
}

@InProceedings{BoseSentiment,
author="Bose, Rajesh and Dey, Raktim Kumar and Roy, Sandip and Sarddar, Debabrata",
editor="Tuba, Milan and Akashe, Shyam and Joshi, Amit",
title="Sentiment Analysis on Online Product Reviews",
booktitle="Information and Communication Technology for Sustainable Development",
year="2020",
publisher="Springer Singapore",
address="Singapore",
pages="559--569",
isbn="978-981-13-7166-0",
doi = {10.1007/978-981-13-7166-0_56},
url = {https://doi.org/10.1007/978-981-13-7166-0_56},
}

@article{Wang12022016,
author = {Hongwei Wang and Lijuan Zheng and},
title = {Sentiment classification of Chinese online reviews: a comparison of factors influencing performances},
journal = {Enterprise Information Systems},
volume = {10},
number = {2},
pages = {228--244},
year = {2016},
publisher = {Taylor \& Francis},
doi = {10.1080/17517575.2014.947635},
URL = {https://doi.org/10.1080/17517575.2014.947635},
eprint = {https://doi.org/10.1080/17517575.2014.947635}
}

@inproceedings{Baj-Rogowska,
author = {Baj-Rogowska, Anna},
year = {2017},
month = {12},
pages = {391-395},
title = {Sentiment analysis of Facebook posts: The Uber case},
doi = {10.1109/INTELCIS.2017.8260068}
}

@INPROCEEDINGS{9574641,
  author={Gupta, Rahul and Sameer, Syed and Muppavarapu, Harsha and Enduri, Murali Krishna and Anamalamudi, Satish},
  booktitle={2021 13th International Conference on Computational Intelligence and Communication Networks (CICN)}, 
  title={Sentiment Analysis on Zomato Reviews}, 
  year={2021},
  volume={},
  number={},
  pages={34-38},
  keywords={Sentiment analysis;Data analysis;Machine learning algorithms;Communication networks;Computational intelligence;sentiment analysis;EDA;zomato;reviews},
  doi={10.1109/CICN51697.2021.9574641}}

@article{BIRJALI2021107134,
title = {A comprehensive survey on sentiment analysis: Approaches, challenges and trends},
journal = {Knowledge-Based Systems},
volume = {226},
pages = {107134},
year = {2021},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2021.107134},
url = {https://www.sciencedirect.com/science/article/pii/S095070512100397X},
author = {Marouane Birjali and Mohammed Kasri and Abderrahim Beni-Hssane},
keywords = {Sentiment analysis, Opinion mining, Machine learning, Lexicon-based, Sentiment classification, Deep learning},
abstract = {Sentiment analysis (SA), also called Opinion Mining (OM) is the task of extracting and analyzing people‚Äôs opinions, sentiments, attitudes, perceptions, etc., toward different entities such as topics, products, and services. The fast evolution of Internet-based applications like websites, social networks, and blogs, leads people to generate enormous heaps of opinions and reviews about products, services, and day-to-day activities. Sentiment analysis poses as a powerful tool for businesses, governments, and researchers to extract and analyze public mood and views, gain business insight, and make better decisions. This paper presents a complete study of sentiment analysis approaches, challenges, and trends, to give researchers a global survey on sentiment analysis and its related fields. The paper presents the applications of sentiment analysis and describes the generic process of this task. Then, it reviews, compares, and investigates the used approaches to have an exhaustive view of their advantages and drawbacks. The challenges of sentiment analysis are discussed next to clarify future directions.}
}

@Article{TadleFOMC,
  author={Tadle, Raul Cruz},
  title={{FOMC minutes sentiments and their impact on financial markets}},
  journal={Journal of Economics and Business},
  year=2022,
  volume={118},
  number={C},
  pages={},
  month={},
  keywords={Federal Reserve; Policy documents; Sentiment analysis; Monetary policy; Asset prices; FOMC},
  doi={10.1016/j.jeconbus.2021.1},
  abstract={I develop a semi-automated method that systematically evaluates the information in FOMC meeting documents. This method highlights economic conditions and calculates document sentiment indices that proxy for the FOMC's interest rate tilt. I compare the sentiment indices of FOMC minutes and their corresponding FOMC statements. Using high-frequency data, I then examine how these sentiment indices are associated with the fed funds futures contracts, broad equity, and real estate investment trust indices, and exchange rate valuation of several major currencies against the U.S. Dollar. I find that minutes sentiments have a statistically significant association with the fed funds futures rate and the U.S. dollar's valuation.},
  url={https://ideas.repec.org/a/eee/jebusi/v118y2022ics0148619521000394.html}
}

@article{DohHowtoSay,
author = {Doh, Taeyoung and Kim, Sungil and Yang, Shu-Kuei},
year = {2021},
month = {02},
pages = {},
title = {How You Say It Matters: Text Analysis of FOMC Statements Using Natural Language Processing},
journal = {The Federal Reserve Bank of Kansas City Economic Review},
doi = {10.18651/ER/v106n1DohKimYang},
volume = {106},
number = {1},
}

@Article{Xing2018,
author={Xing, Frank Z. and Cambria, Erik and Welsch, Roy E.},
title={Natural language based financial forecasting: a survey},
journal={Artificial Intelligence Review},
year={2018},
month={6},
day={01},
volume={50},
number={1},
pages={49-73},
abstract={Natural language processing (NLP), or the pragmatic research perspective of computational linguistics, has become increasingly powerful due to data availability and various techniques developed in the past decade. This increasing capability makes it possible to capture sentiments more accurately and semantics in a more nuanced way. Naturally, many applications are starting to seek improvements by adopting cutting-edge NLP techniques. Financial forecasting is no exception. As a result, articles that leverage NLP techniques to predict financial markets are fast accumulating, gradually establishing the research field of natural language based financial forecasting (NLFF), or from the application perspective, stock market prediction. This review article clarifies the scope of NLFF research by ordering and structuring techniques and applications from related work. The survey also aims to increase the understanding of progress and hotspots in NLFF, and bring about discussions across many different disciplines.},
issn={1573-7462},
doi={10.1007/s10462-017-9588-9},
url={https://doi.org/10.1007/s10462-017-9588-9}
}

@article{ROGNONE2020101462,
title = {News sentiment in the cryptocurrency market: An empirical comparison with Forex},
journal = {International Review of Financial Analysis},
volume = {69},
pages = {101462},
year = {2020},
issn = {1057-5219},
doi = {https://doi.org/10.1016/j.irfa.2020.101462},
url = {https://www.sciencedirect.com/science/article/pii/S105752192030106X},
author = {Lavinia Rognone and Stuart Hyde and S. Sarah Zhang},
}

@article{KRAAIJEVELD2020101188,
title = {The predictive power of public Twitter sentiment for forecasting cryptocurrency prices},
journal = {Journal of International Financial Markets, Institutions and Money},
volume = {65},
pages = {101188},
year = {2020},
issn = {1042-4431},
doi = {https://doi.org/10.1016/j.intfin.2020.101188},
url = {https://www.sciencedirect.com/science/article/pii/S104244312030072X},
author = {Olivier Kraaijeveld and Johannes {De Smedt}},
}

@article{ManguriCovid,
author = {Manguri, Kamaran and Ramadhan, Rebaz and Mohammed Amin, Pshko},
year = {2020},
month = {05},
pages = {54-65},
title = {Twitter Sentiment Analysis on Worldwide COVID-19 Outbreaks},
volume = {5},
journal = {Kurdistan Journal of Applied Research},
doi = {10.24017/covid.8}
}

@article{NaseemCovid,
author = {Naseem, Usman and Razzak, Imran and Khushi, Matloob and Eklund, Peter and Kim, Jinman},
year = {2021},
month = {01},
pages = {1-13},
title = {COVIDSenti: A Large-Scale Benchmark Twitter Data Set for COVID-19 Sentiment Analysis},
volume = {PP},
journal = {IEEE Transactions on Computational Social Systems},
doi = {10.1109/TCSS.2021.3051189}
}

@article{CHAKRABORTY2020106754,
title = {Sentiment Analysis of COVID-19 tweets by Deep Learning Classifiers‚ÄîA study to show how popularity is affecting accuracy in social media},
journal = {Applied Soft Computing},
volume = {97},
pages = {106754},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106754},
url = {https://www.sciencedirect.com/science/article/pii/S156849462030692X},
author = {Koyel Chakraborty and Surbhi Bhatia and Siddhartha Bhattacharyya and Jan Platos and Rajib Bag and Aboul Ella Hassanien},
}

@article{ALAMOODI2021114155,
title = {Sentiment analysis and its applications in fighting COVID-19 and infectious diseases: A systematic review},
journal = {Expert Systems with Applications},
volume = {167},
pages = {114155},
year = {2021},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2020.114155},
url = {https://www.sciencedirect.com/science/article/pii/S0957417420308988},
author = {A.H. Alamoodi and B.B. Zaidan and A.A. Zaidan and O.S. Albahri and K.I. Mohammed and R.Q. Malik and E.M. Almahdi and M.A. Chyad and Z. Tareq and A.S. Albahri and Hamsa Hameed and Musaab Alaa},
}

@Article{Abd-Alrazaq,
author="Abd-Alrazaq, Alaa
and Alhuwail, Dari
and Househ, Mowafa
and Hamdi, Mounir
and Shah, Zubair",
title="Top Concerns of Tweeters During the COVID-19 Pandemic: Infoveillance Study",
journal="J Med Internet Res",
year="2020",
month="4",
day="21",
volume="22",
number="4",
pages="e19016",
issn="1438-8871",
doi="10.2196/19016",
url="http://www.jmir.org/2020/4/e19016/",
url="https://doi.org/10.2196/19016",
url="http://www.ncbi.nlm.nih.gov/pubmed/32287039"
}

@article{baker2023prediction,
  title={Prediction of people sentiments on Twitter using machine learning classifiers during Russian-Ukrainian conflict},
  author={Baker, Mohammed Rashad and Taher, Yalmaz Najmaldin and Jihad, Kamal h.},
  year={2023},
  doi = {10.21203/rs.3.rs-2410016/v1},
  url = {https://doi.org/10.21203/rs.3.rs-2410016/v1},
  month = {1},
  version = {1},
}

@InProceedings{JihadEV,
author="Jihad, Kamal H. and Baker, Mohammed Rashad and Farhat, Mariem and Frikha, Mondher",
editor="Abraham, Ajith and Hong, Tzung-Pei and Kotecha, Ketan and Ma, Kun and Manghirmalani Mishra, Pooja and Gandhi, Niketa",
title="Machine Learning-Based Social Media¬†Text Analysis: Impact of the Rising Fuel Prices on Electric Vehicles",
booktitle="Hybrid Intelligent Systems",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="625--635",
isbn="978-3-031-27409-1",
doi = {10.1007/978-3-031-27409-1_57},
}

@article{SHEN2019249,
title = {Sentiment based matrix factorization with reliability for recommendation},
journal = {Expert Systems with Applications},
volume = {135},
pages = {249-258},
year = {2019},
issn = {0957-4174},
doi = {https://doi.org/10.1016/j.eswa.2019.06.001},
url = {https://www.sciencedirect.com/science/article/pii/S0957417419303951},
author = {Rong-Ping Shen and Heng-Ru Zhang and Hong Yu and Fan Min},
}

@article{LI2016164,
title = {An intelligent movie recommendation system through group-level sentiment analysis in microblogs},
journal = {Neurocomputing},
volume = {210},
pages = {164-173},
year = {2016},
note = {SI:Behavior Analysis In SN},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2015.09.134},
url = {https://www.sciencedirect.com/science/article/pii/S0925231216305872},
author = {Hui Li and Jiangtao Cui and Bingqing Shen and Jianfeng Ma},
}

@article{SERRANOGUERRERO2020106768,
title = {A T1OWA and aspect-based model for customizing recommendations on eCommerce},
journal = {Applied Soft Computing},
volume = {97},
pages = {106768},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106768},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620307067},
author = {Jesus Serrano-Guerrero and Jose A. Olivas and Francisco P. Romero},
}

@article{RAY2021106935,
title = {An ensemble-based hotel recommender system using sentiment analysis and aspect categorization of hotel reviews},
journal = {Applied Soft Computing},
volume = {98},
pages = {106935},
year = {2021},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106935},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620308735},
author = {Biswarup Ray and Avishek Garain and Ram Sarkar},
}

@article{FU2020106803,
title = {A product ranking method combining the features--opinion pairs mining and interval-valued Pythagorean fuzzy sets},
journal = {Applied Soft Computing},
volume = {97},
pages = {106803},
year = {2020},
issn = {1568-4946},
doi = {https://doi.org/10.1016/j.asoc.2020.106803},
url = {https://www.sciencedirect.com/science/article/pii/S1568494620307419},
author = {Xiangling Fu and Tianxiong Ouyang and Zaoli Yang and Shaohui Liu},
}

@Article{Ayata2020,
author={Ayata, De{\u{g}}er
and Yaslan, Yusuf
and Kamasak, Mustafa E.},
title={Emotion Recognition from Multimodal Physiological Signals for Emotion Aware Healthcare Systems},
journal={Journal of Medical and Biological Engineering},
year={2020},
month={4},
day={01},
volume={40},
number={2},
pages={149-157},
issn={2199-4757},
doi={10.1007/s40846-019-00505-7},
url={https://doi.org/10.1007/s40846-019-00505-7}
}

@article{JIMENEZZAFRA201950,
title = {How do we talk about doctors and drugs? Sentiment analysis in forums expressing opinions for medical domain},
journal = {Artificial Intelligence in Medicine},
volume = {93},
pages = {50-57},
year = {2019},
note = {Extracting and Processing of Rich Semantics from Medical Texts},
issn = {0933-3657},
doi = {https://doi.org/10.1016/j.artmed.2018.03.007},
url = {https://www.sciencedirect.com/science/article/pii/S0933365717305407},
author = {Salud Mar√≠a Jim√©nez-Zafra and M. Teresa Mart√≠n-Valdivia and M. Dolores Molina-Gonz√°lez and L. Alfonso Ure√±a-L√≥pez},
}

@Inbook{Ram√≠rez-Tinoco2019,
author="Ram{\'i}rez-Tinoco, Francisco Javier
and Alor-Hern{\'a}ndez, Giner
and S{\'a}nchez-Cervantes, Jos{\'e} Luis
and Salas-Z{\'a}rate, Mar{\'i}a del Pilar
and Valencia-Garc{\'i}a, Rafael",
editor="Alor-Hern{\'a}ndez, Giner
and S{\'a}nchez-Cervantes, Jos{\'e} Luis
and Rodr{\'i}guez-Gonz{\'a}lez, Alejandro
and Valencia-Garc{\'i}a, Rafael",
title="Use of Sentiment Analysis Techniques in Healthcare Domain",
bookTitle="Current Trends in Semantic Web Technologies: Theory and Practice",
year="2019",
publisher="Springer International Publishing",
address="Cham",
pages="189--212",
isbn="978-3-030-06149-4",
doi="10.1007/978-3-030-06149-4_8",
url="https://doi.org/10.1007/978-3-030-06149-4_8"
}

@article{Desmet2013,
author = {Desmet, Bart and Hoste, V√©ronique},
year = {2013},
month = {11},
pages = {6351‚Äì6358},
title = {Emotion detection in suicide notes},
volume = {40},
journal = {Expert Systems with Applications},
doi = {10.1016/j.eswa.2013.05.050}
}

@InProceedings{Wang2013,
author="Wang, Xinyu and Zhang, Chunhong and Ji, Yang and Sun, Li and Wu, Leijia and Bao, Zhana",
editor="Li, Jiuyong and Cao, Longbing and Wang, Can and Tan, Kay Chen and Liu, Bo and Pei, Jian and Tseng, Vincent S.",
title="A Depression Detection Model Based on Sentiment Analysis in Micro-blog Social Network",
booktitle="Trends and Applications in Knowledge Discovery and Data Mining",
year="2013",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="201--213",
isbn="978-3-642-40319-4",
doi = {10.1007/978-3-642-40319-4_18},
url = {https://doi.org/10.1007/978-3-642-40319-4_18},
}

@misc{clark2018sentimentanalysisbreastcancer,
title={A Sentiment Analysis of Breast Cancer Treatment Experiences and Healthcare Perceptions Across Twitter}, 
author={Eric M. Clark and Ted James and Chris A. Jones and Amulya Alapati and Promise Ukandu and Christopher M. Danforth and Peter Sheridan Dodds},
year={2018},
eprint={1805.09959},
archivePrefix={arXiv},
primaryClass={cs.CL},
url={https://arxiv.org/abs/1805.09959}, 
}

@Article{Jung2017,
author="Jung, Hyesil and Park, Hyeoun-Ae and Song, Tae-Min",
title="Ontology-Based Approach to Social Data Sentiment Analysis: Detection of Adolescent Depression Signals",
journal="J Med Internet Res",
year="2017",
month="7",
day="24",
volume="19",
number="7",
pages="e259",
issn="1438-8871",
doi="10.2196/jmir.7452",
url="http://www.jmir.org/2017/7/e259/",
url="https://doi.org/10.2196/jmir.7452",
url="http://www.ncbi.nlm.nih.gov/pubmed/28739560"
}

@InProceedings{Baker2023,
author="Baker, Mohammed Rashad and Mohammed, Esraa Zeki and Jihad, Kamal H.",
editor="Abraham, Ajith and Pllana, Sabri and Casalino, Gabriella and Ma, Kun and Bajaj, Anu",
title="Prediction of Colon Cancer Related Tweets Using Deep Learning Models",
booktitle="Intelligent Systems Design and Applications",
year="2023",
publisher="Springer Nature Switzerland",
address="Cham",
pages="522--532",
isbn="978-3-031-27440-4",
doi = {10.1007/978-3-031-27440-4_50},
url = {https://doi.org/10.1007/978-3-031-27440-4_50},
}

@inproceedings{Jiang2019,
author = {Jiang, Lin and Suzuki, Yoshimi},
year = {2019},
month = {11},
pages = {671-676},
title = {Detecting hate speech from tweets for sentiment analysis},
doi = {10.1109/ICSAI48974.2019.9010578}
}

@inproceedings{Badjatiya_2017, 
series={WWW ‚Äô17 Companion},
title={Deep Learning for Hate Speech Detection in Tweets},
url={http://dx.doi.org/10.1145/3041021.3054223},
DOI={10.1145/3041021.3054223},
booktitle={Proceedings of the 26th International Conference on World Wide Web Companion - WWW ‚Äô17 Companion},
publisher={ACM Press},
author={Badjatiya, Pinkesh and Gupta, Shashank and Gupta, Manish and Varma, Vasudeva},
year={2017},
pages={759‚Äì760},
collection={WWW ‚Äô17 Companion} }

@INPROCEEDINGS{8669073,
  author={Rodr√≠guez, Axel and Argueta, Carlos and Chen, Yi-Ling},
  booktitle={2019 International Conference on Artificial Intelligence in Information and Communication (ICAIIC)}, 
  title={Automatic Detection of Hate Speech on Facebook Using Sentiment and Emotion Analysis}, 
  year={2019},
  volume={},
  number={},
  pages={169-174},
  keywords={Facebook;Jamming;Compounds;Clustering algorithms;Dictionaries;Sentiment analysis;Hate speech;Facebook;sentiment analysis;clustering},
  doi={10.1109/ICAIIC.2019.8669073}}

@article{Zidny2019,
author = {Zidny Naf'an, Muhammad and Bimantara, Alhamda and Larasati, Afiatari and Risondang, Ezar and Nugraha, Novanda},
year = {2019},
month = {04},
pages = {88-98},
title = {Sentiment Analysis of Cyberbullying on Instagram User Comments},
volume = {2},
journal = {Journal of Data Science and Its Applications},
doi = {10.21108/jdsa.2019.2.20}
}

@inproceedings{pontiki-etal-2014-semeval,
title = "{S}em{E}val-2014 Task 4: Aspect Based Sentiment Analysis",
author = "Pontiki, Maria  and
  Galanis, Dimitris  and
  Pavlopoulos, John  and
  Papageorgiou, Harris  and
  Androutsopoulos, Ion  and
  Manandhar, Suresh",
editor = "Nakov, Preslav  and
  Zesch, Torsten",
booktitle = "Proceedings of the 8th International Workshop on Semantic Evaluation ({S}em{E}val 2014)",
month = "8",
year = "2014",
address = "Dublin, Ireland",
publisher = "Association for Computational Linguistics",
url = "https://aclanthology.org/S14-2004/",
doi = "10.3115/v1/S14-2004",
pages = "27--35"
}

@inproceedings{jiang-etal-2019-challenge,
    title = "A Challenge Dataset and Effective Models for Aspect-Based Sentiment Analysis",
    author = "Jiang, Qingnan and Chen, Lei and Xu, Ruifeng and Ao, Xiang and Yang, Min",
    editor = "Inui, Kentaro and Jiang, Jing and Ng, Vincent and Wan, Xiaojun",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = "11",
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-1654/",
    doi = "10.18653/v1/D19-1654",
    pages = "6280--6285",
}

@inproceedings{li-etal-2019-exploiting,
    title = "Exploiting {BERT} for End-to-End Aspect-based Sentiment Analysis",
    author = "Li, Xin and Bing, Lidong and Zhang, Wenxuan and Lam, Wai",
    editor = "Xu, Wei and Ritter, Alan and Baldwin, Tim and Rahimi, Afshin",
    booktitle = "Proceedings of the 5th Workshop on Noisy User-generated Text (W-NUT 2019)",
    month = "11",
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D19-5505/",
    doi = "10.18653/v1/D19-5505",
    pages = "34--41",
}

@inproceedings{toledo-ronen-etal-2022-multi,
    title = "Multi-Domain Targeted Sentiment Analysis",
    author = "Toledo-Ronen, Orith and Orbach, Matan and Katz, Yoav and Slonim, Noam",
    editor = "Carpuat, Marine and de Marneffe, Marie-Catherine and Meza Ruiz, Ivan Vladimir",
    booktitle = "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = "7",
    year = "2022",
    address = "Seattle, United States",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.naacl-main.198/",
    doi = "10.18653/v1/2022.naacl-main.198",
    pages = "2751--2762",
}

@article{LIU2023110339,
title = {Enhancing aspect-category sentiment analysis via syntactic data augmentation and knowledge enhancement},
journal = {Knowledge-Based Systems},
volume = {264},
pages = {110339},
year = {2023},
issn = {0950-7051},
doi = {https://doi.org/10.1016/j.knosys.2023.110339},
url = {https://www.sciencedirect.com/science/article/pii/S0950705123000898},
author = {Bin Liu and Tao Lin and Ming Li},
}
@inproceedings{zhang-etal-2022-boundary,
    title = "Boundary-Driven Table-Filling for Aspect Sentiment Triplet Extraction",
    author = "Zhang, Yice and Yang, Yifan and Li, Yihui and Liang, Bin and Chen, Shiwei and Dang, Yixue and Yang, Min and Xu, Ruifeng",
    editor = "Goldberg, Yoav and Kozareva, Zornitsa and Zhang, Yue",
    booktitle = "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing",
    month = "12",
    year = "2022",
    address = "Abu Dhabi, United Arab Emirates",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.emnlp-main.435/",
    doi = "10.18653/v1/2022.emnlp-main.435",
    pages = "6485--6498",
}

@misc{wu2020contextguidedberttargetedaspectbased,
      title={Context-Guided BERT for Targeted Aspect-Based Sentiment Analysis}, 
      author={Zhengxuan Wu and Desmond C. Ong},
      year={2020},
      eprint={2010.07523},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2010.07523}, 
}

@inproceedings{mukherjee-etal-2021-paste,
    title = "{PASTE}: A Tagging-Free Decoding Framework Using Pointer Networks for Aspect Sentiment Triplet Extraction",
    author = "Mukherjee, Rajdeep and Nayak, Tapas and Butala, Yash and Bhattacharya, Sourangshu and Goyal, Pawan",
    editor = "Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau",
    booktitle = "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing",
    month = "11",
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.emnlp-main.731/",
    doi = "10.18653/v1/2021.emnlp-main.731",
    pages = "9279--9291",
}

@inproceedings{sun-etal-2019-utilizing,
    title = "Utilizing {BERT} for Aspect-Based Sentiment Analysis via Constructing Auxiliary Sentence",
    author = "Sun, Chi and Huang, Luyao and Qiu, Xipeng",
    editor = "Burstein, Jill and Doran, Christy and Solorio, Thamar",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/N19-1035/",
    doi = "10.18653/v1/N19-1035",
    pages = "380--385",
}

@article{Wan_Yang_Du_Liu_Qi_Pan_2020, 
title={Target-Aspect-Sentiment Joint Detection for Aspect-Based Sentiment Analysis}, volume={34}, 
url={https://ojs.aaai.org/index.php/AAAI/article/view/6447}, 
DOI={10.1609/aaai.v34i05.6447}, 
number={05}, 
journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
author={Wan, Hai and Yang, Yufei and Du, Jianfeng and Liu, Yanan and Qi, Kunxun and Pan, Jeff Z.}, 
year={2020}, 
month={Apr.}, 
pages={9122-9129} }

@Article{Ke2023,
author={Ke, Cai and Xiong, Qingyu and Wu, Chao and Yi, Hualing and Gao, Min and Chen, Jie},
title={SimCPD: a simple framework for contrastive prompts of target-aspect-sentiment joint detection},
journal={Neural Computing and Applications},
year={2023},
month={Aug},
day={01},
volume={35},
number={22},
pages={16577-16592},
issn={1433-3058},
doi={10.1007/s00521-023-08529-6},
url={https://doi.org/10.1007/s00521-023-08529-6}
}

@ARTICLE{Li2023-wo,
  title = {An aspect-category-opinion-sentiment quadruple extraction with distance information for implicit sentiment analysis},
  author = {Li, Jianwei and Li, Xianyong and Du, Yajun and Fan, Yongquan and Chen, Xiaoliang and Huang, Dong},
  journal  = {Inf. Technol. Contr.},
  publisher = {Kaunas University of Technology (KTU)},
  volume = {52},
  number = {2},
  pages = {445--456},
  month = {7},
  year = {2023},
  doi = {10.5755/j01.itc.52.2.32903},
  url = {https://doi.org/10.5755/j01.itc.52.2.32903},
}

@inproceedings{steinberger-etal-2014-aspect,
    title = "Aspect-Level Sentiment Analysis in {C}zech",
    author = "Steinberger, Josef  and
      Brychc{\'i}n, Tom{\'a}{\v{s}}  and
      Konkol, Michal",
    editor = "Balahur, Alexandra  and
      van der Goot, Erik  and
      Steinberger, Ralf  and
      Montoyo, Andres",
    booktitle = "Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    month = jun,
    year = "2014",
    address = "Baltimore, Maryland",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W14-2605/",
    doi = "10.3115/v1/W14-2605",
    pages = "24--30"
}

@book{veselovska-2017,
title = {Sentiment analysis in Czech},
author = {Kate\v{r}ina Veselovsk\'{a}},
year = {2017},
publisher = {{\'{U}}{FAL}},
organization = {Univerzita Karlova},
address = {Praha, Czechia},
series = {Studies in Computational and Theoretical Linguistics},
volume = {16},
isbn = {978-80-88132-03-5},
url = {https://ufal.mff.cuni.cz/books/preview/2017-veselovska_full.pdf},
}

@misc{sido2021czertczechbertlike,
      title={Czert -- Czech BERT-like Model for Language Representation}, 
      author={Jakub Sido and Ond≈ôej Pra≈æ√°k and Pavel P≈ôib√°≈à and Jan Pa≈°ek and Michal Sej√°k and Miloslav Konop√≠k},
      year={2021},
      eprint={2103.13031},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2103.13031}, 
}

@inproceedings{_ano_2019,
   title={Sentiment Analysis of Czech Texts: An Algorithmic Survey},
   url={http://dx.doi.org/10.5220/0007695709730979},
   DOI={10.5220/0007695709730979},
   booktitle={Proceedings of the 11th International Conference on Agents and Artificial Intelligence},
   publisher={SCITEPRESS - Science and Technology Publications},
   author={√áano, Erion and Bojar, Ond≈ôej},
   year={2019} }

@inproceedings{pontiki-etal-2016-semeval,
    title = "{S}em{E}val-2016 Task 5: Aspect Based Sentiment Analysis",
    author = {Pontiki, Maria  and
      Galanis, Dimitris  and
      Papageorgiou, Haris  and
      Androutsopoulos, Ion  and
      Manandhar, Suresh  and
      AL-Smadi, Mohammad  and
      Al-Ayyoub, Mahmoud  and
      Zhao, Yanyan  and
      Qin, Bing  and
      De Clercq, Orph{\'e}e  and
      Hoste, V{\'e}ronique  and
      Apidianaki, Marianna  and
      Tannier, Xavier  and
      Loukachevitch, Natalia  and
      Kotelnikov, Evgeniy  and
      Bel, Nuria  and
      Jim{\'e}nez-Zafra, Salud Mar{\'i}a  and
      Eryi{\u{g}}it, G{\"u}l{\c{s}}en},
    editor = "Bethard, Steven  and
      Carpuat, Marine  and
      Cer, Daniel  and
      Jurgens, David  and
      Nakov, Preslav  and
      Zesch, Torsten",
    booktitle = "Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016)",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/S16-1002/",
    doi = "10.18653/v1/S16-1002",
    pages = "19--30"
}

@inproceedings{habernal-etal-2013-sentiment,
    title = "Sentiment Analysis in {C}zech Social Media Using Supervised Machine Learning",
    author = "Habernal, Ivan  and
      Pt{\'a}{\v{c}}ek, Tom{\'a}{\v{s}}  and
      Steinberger, Josef",
    editor = "Balahur, Alexandra  and
      van der Goot, Erik  and
      Montoyo, Andres",
    booktitle = "Proceedings of the 4th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",
    month = jun,
    year = "2013",
    address = "Atlanta, Georgia",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W13-1609/",
    pages = "65--74"
}

@inproceedings{priban-prazak-2023-improving,
    title = "Improving Aspect-Based Sentiment with End-to-End Semantic Role Labeling Model",
    author = "P{\v{r}}ib{\'a}{\v{n}}, Pavel  and
      Prazak, Ondrej",
    editor = "Mitkov, Ruslan  and
      Angelova, Galia",
    booktitle = "Proceedings of the 14th International Conference on Recent Advances in Natural Language Processing",
    month = sep,
    year = "2023",
    address = "Varna, Bulgaria",
    publisher = "INCOMA Ltd., Shoumen, Bulgaria",
    url = "https://aclanthology.org/2023.ranlp-1.96/",
    pages = "888--897",
}

@misc{clark2020electrapretrainingtextencoders,
      title={ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators}, 
      author={Kevin Clark and Minh-Thang Luong and Quoc V. Le and Christopher D. Manning},
      year={2020},
      eprint={2003.10555},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2003.10555}, 
}

@inproceedings{smid-etal-2024-czech,
    title = "{C}zech Dataset for Complex Aspect-Based Sentiment Analysis Tasks",
    author = "{\v{S}}m{\'i}d, Jakub  and
      P{\v{r}}ib{\'a}{\v{n}}, Pavel  and
      Prazak, Ondrej  and
      Kral, Pavel",
    editor = "Calzolari, Nicoletta  and
      Kan, Min-Yen  and
      Hoste, Veronique  and
      Lenci, Alessandro  and
      Sakti, Sakriani  and
      Xue, Nianwen",
    booktitle = "Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024)",
    month = may,
    year = "2024",
    address = "Torino, Italia",
    publisher = "ELRA and ICCL",
    url = "https://aclanthology.org/2024.lrec-main.384/",
    pages = "4299--4310",
}

@misc{lan2020albertlitebertselfsupervised,
      title={ALBERT: A Lite BERT for Self-supervised Learning of Language Representations}, 
      author={Zhenzhong Lan and Mingda Chen and Sebastian Goodman and Kevin Gimpel and Piyush Sharma and Radu Soricut},
      year={2020},
      eprint={1909.11942},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1909.11942}, 
}

@misc{zhang2022surveyaspectbasedsentimentanalysis,
      title={A Survey on Aspect-Based Sentiment Analysis: Tasks, Methods, and Challenges}, 
      author={Wenxuan Zhang and Xin Li and Yang Deng and Lidong Bing and Wai Lam},
      year={2022},
      eprint={2203.01054},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2203.01054}, 
}

@online{TomLaptops,
  author    = {Tom Aarsen},
  title     = {Dataset Card for "tomaarsen/setfit-absa-semeval-laptops"},
  year      = {2023},
  url       = {https://huggingface.co/datasets/tomaarsen/setfit-absa-semeval-laptops},
  urldate   = {2025-04-15},
}

@online{TomRestaurants,
  author    = {Tom Aarsen},
  title     = {Dataset Card for "tomaarsen/setfit-absa-semeval-restaurants"},
  year      = {2023},
  url       = {https://huggingface.co/datasets/tomaarsen/setfit-absa-semeval-restaurants},
  urldate   = {2025-04-15},
}

@online{CzechRestaurants,
  author = "Steinberger, Josef  and
      Brychc{\'i}n, Tom{\'a}{\v{s}}  and
      Konkol, Michal",
  title     = {Restaurant Reviews CZ ABSA},
  year      = {2014},
  url       = {https://corpora.kiv.zcu.cz/sentiment/},
  urldate   = {2025-04-16},
  note = {P≈ô√≠m√Ω link: https://corpora.kiv.zcu.cz/sentiment/CzechABSA-v2.zip}
}

@online{BERTbaseuncased,
  author    = {Julien Chaumond and Omar Sanseviero and Lysandre},
  title     = {BERT base model (uncased)},
  year      = {2024},
  url       = {https://huggingface.co/google-bert/bert-base-uncased},
  urldate   = {2025-04-25},
}

@online{BERTlargeuncased,
  author    = {Julien Chaumond and Omar Sanseviero and Lysandre},
  title     = {BERT large model (uncased)},
  year      = {2024},
  url       = {https://huggingface.co/google-bert/bert-large-uncased},
  urldate   = {2025-04-25},
}

@online{BERTbasecased,
  author    = {Julien Chaumond and Omar Sanseviero and Lysandre},
  title     = {BERT base model (cased)},
  year      = {2024},
  url       = {https://huggingface.co/google-bert/bert-base-cased},
  urldate   = {2025-04-25},
}

@online{BERTlargecased,
  author    = {Julien Chaumond and Omar Sanseviero and Lysandre},
  title     = {BERT large model (cased)},
  year      = {2024},
  url       = {https://huggingface.co/google-bert/bert-large-cased},
  urldate   = {2025-04-25},
}

@online{mBERTbasecased,
  author    = {Julien Chaumond and Omar Sanseviero and Lysandre},
  title     = {BERT multilingual base model (cased)},
  year      = {2024},
  url       = {https://huggingface.co/google-bert/bert-base-multilingual-cased},
  urldate   = {2025-04-26},
}

@online{GoogleResearchGitHubMulti,
  author    = {Google-Research},
  title     = {Google Research github - BERT - bert/multilingual.md},
  year      = {2024},
  url       = {https://github.com/google-research/bert/blob/master/multilingual.md},
  urldate   = {2025-04-26},
}

@online{RoBERTabase,
  author    = {Julien Chaumond and Lysandre},
  title     = {RoBERTa base model},
  year      = {2024},
  url       = {https://huggingface.co/FacebookAI/roberta-base},
  urldate   = {2025-04-26},
}

@online{RoBERTalarge,
  author    = {Julien Chaumond and Lysandre},
  title     = {RoBERTa large model},
  year      = {2024},
  url       = {https://huggingface.co/FacebookAI/roberta-large},
  urldate   = {2025-04-26},
}

@misc{conneau2020unsupervisedcrosslingualrepresentationlearning,
      title={Unsupervised Cross-lingual Representation Learning at Scale}, 
      author={Alexis Conneau and Kartikay Khandelwal and Naman Goyal and Vishrav Chaudhary and Guillaume Wenzek and Francisco Guzm√°n and Edouard Grave and Myle Ott and Luke Zettlemoyer and Veselin Stoyanov},
      year={2020},
      eprint={1911.02116},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1911.02116}, 
}

@misc{lample2019crosslinguallanguagemodelpretraining,
      title={Cross-lingual Language Model Pretraining}, 
      author={Guillaume Lample and Alexis Conneau},
      year={2019},
      eprint={1901.07291},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/1901.07291}, 
}

@online{XLMRoBERTabase,
  author    = {Julien Chaumond and Lysandre},
  title     = {XLM-RoBERTa (base-sized model)},
  year      = {2024},
  url       = {https://huggingface.co/FacebookAI/xlm-roberta-base},
  urldate   = {2025-04-26},
}

@online{XLMRoBERTalarge,
  author    = {Julien Chaumond and Lysandre},
  title     = {XLM-RoBERTa (large-sized model)},
  year      = {2024},
  url       = {https://huggingface.co/FacebookAI/xlm-roberta-large},
  urldate   = {2025-04-26},
}

@misc{he2021debertadecodingenhancedbertdisentangled,
      title={DeBERTa: Decoding-enhanced BERT with Disentangled Attention}, 
      author={Pengcheng He and Xiaodong Liu and Jianfeng Gao and Weizhu Chen},
      year={2021},
      eprint={2006.03654},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2006.03654}, 
}

@misc{he2023debertav3improvingdebertausing,
      title={DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing}, 
      author={Pengcheng He and Jianfeng Gao and Weizhu Chen},
      year={2023},
      eprint={2111.09543},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2111.09543}, 
}

@online{DeBERTav3base,
  author    = {Microsoft},
  title     = {DeBERTaV3: Improving DeBERTa using ELECTRA-Style Pre-Training with Gradient-Disentangled Embedding Sharing},
  year      = {2023},
  url       = {https://huggingface.co/microsoft/deberta-v3-base},
  urldate   = {2025-04-26},
}

@online{ModernBERTbase,
  author={Benjamin Warner and Antoine Chaffin and Benjamin Clavi√© and Orion Weller and Oskar Hallstr√∂m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
  title     = {ModernBERT},
  year      = {2024},
  url       = {https://huggingface.co/answerdotai/ModernBERT-base},
  urldate   = {2025-04-26},
}

@online{ModernBERTlarge,
  author={Benjamin Warner and Antoine Chaffin and Benjamin Clavi√© and Orion Weller and Oskar Hallstr√∂m and Said Taghadouini and Alexis Gallagher and Raja Biswas and Faisal Ladhak and Tom Aarsen and Nathan Cooper and Griffin Adams and Jeremy Howard and Iacopo Poli},
  title     = {ModernBERT},
  year      = {2024},
  url       = {https://huggingface.co/answerdotai/ModernBERT-large},
  urldate   = {2025-04-26},
}

@inproceedings{arkhipov-etal-2019-tuning,
    title = "Tuning Multilingual Transformers for Language-Specific Named Entity Recognition",
    author = "Arkhipov, Mikhail  and
      Trofimova, Maria  and
      Kuratov, Yuri  and
      Sorokin, Alexey",
    editor = "Erjavec, Toma{\v{z}}  and
      Marci{\'n}czuk, Micha{\l}  and
      Nakov, Preslav  and
      Piskorski, Jakub  and
      Pivovarova, Lidia  and
      {\v{S}}najder, Jan  and
      Steinberger, Josef  and
      Yangarber, Roman",
    booktitle = "Proceedings of the 7th Workshop on Balto-Slavic Natural Language Processing",
    month = aug,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W19-3712/",
    doi = "10.18653/v1/W19-3712",
    pages = "89--93",
    abstract = "Our paper addresses the problem of multilingual named entity recognition on the material of 4 languages: Russian, Bulgarian, Czech and Polish. We solve this task using the BERT model. We use a hundred languages multilingual model as base for transfer to the mentioned Slavic languages. Unsupervised pre-training of the BERT model on these 4 languages allows to significantly outperform baseline neural approaches and multilingual BERT. Additional improvement is achieved by extending BERT with a word-level CRF layer. Our system was submitted to BSNLP 2019 Shared Task on Multilingual Named Entity Recognition and demonstrated top performance in multilingual setting for two competition metrics. We open-sourced NER models and BERT model pre-trained on the four Slavic languages."
}

@online{SlavicBERThug,
  author={{DeepPavlov}},
  title     = {SlavicBERT},
  year      = {2022},
  url       = {https://huggingface.co/DeepPavlov/bert-base-bg-cs-pl-ru-cased},
  urldate   = {2025-04-26},
}

@online{XLMRSlavic,
  author    = {Nikola Ivaƒçiƒç},
  title     = {XLM-Roberta-base NER model for slavic languages},
  year      = {2024},
  url       = {https://huggingface.co/ivlcic/xlmr-ner-slavic},
  urldate   = {2025-04-26},
}

@online{CzertHugging,
  author    = {Ond≈ôej Pra≈æ√°k and Jan Pa≈°ek and 
Jakub Sido},
  title     = {CZERT-B},
  year      = {2022},
  url       = {https://huggingface.co/UWB-AIR/Czert-B-base-cased},
  urldate   = {2025-04-26},
}

@online{GoogleColab,
  author    = {Google},
  title     = {Google Colaboratory},
  year      = {2025},
  url       = {https://colab.google},
  urldate   = {2025-04-27},
}

@online{T4Nvidia,
  author    = {nVidia},
  title     = {NVIDIA T4},
  year      = {2020},
  url       = {https://www.nvidia.com/en-us/data-center/tesla-t4/},
  urldate   = {2025-04-27},
}

@online{HuggDatasets,
  author    = {HuggingFace},
  title     = {Datasets},
  year      = {2025},
  url       = {https://huggingface.co/docs/datasets/index},
  urldate   = {2025-04-27},
}

@online{pandas,
  author    = {pandas},
  title     = {pandas},
  year      = {2024},
  url       = {https://pandas.pydata.org},
  urldate   = {2025-04-27},
}

@online{numpy,
  author    = {NumPy},
  title     = {NumPy},
  year      = {2024},
  url       = {https://numpy.org},
  urldate   = {2025-04-27},
}

@online{HuggTransformers,
  author    = {HuggingFace},
  title     = {Transformers},
  year      = {2025},
  url       = {https://huggingface.co/docs/transformers/en/index},
  urldate   = {2025-04-27},
}

@online{HuggEvaluate,
  author    = {HuggingFace},
  title     = {Evaluate},
  year      = {2025},
  url       = {https://huggingface.co/docs/evaluate/index},
  urldate   = {2025-04-27},
}

@online{PyTorch,
  author    = {PyTorch},
  title     = {PyTorch},
  year      = {2025},
  url       = {https://pytorch.org},
  urldate   = {2025-04-27},
}

@online{HuggTrainArg,
  author    = {HuggingFace},
  title     = {TrainingArguments},
  year      = {2025},
  url       = {https://huggingface.co/docs/transformers/v4.51.3/en/main_classes/trainer#transformers.TrainingArguments},
  urldate   = {2025-04-27},
}

@online{HuggTrainAcc,
  author    = {HuggingFace},
  title     = {Metric: accuracy},
  year      = {2025},
  url       = {https://huggingface.co/spaces/evaluate-metric/accuracy},
  urldate   = {2025-04-27},
}

@online{HuggTrainf1,
  author    = {HuggingFace},
  title     = {Metric: f1},
  year      = {2025},
  url       = {https://huggingface.co/spaces/evaluate-metric/f1},
  urldate   = {2025-04-27},
}

@inbook{Straka_2021,
   title={RobeCzech: Czech RoBERTa, a Monolingual Contextualized Language Representation Model},
   ISBN={9783030835279},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-030-83527-9_17},
   DOI={10.1007/978-3-030-83527-9_17},
   booktitle={Text, Speech, and Dialogue},
   publisher={Springer International Publishing},
   author={Straka, Milan and N√°plava, Jakub and Strakov√°, Jana and Samuel, David},
   year={2021},
   pages={197‚Äì209} }

@inbook{Lehe_ka_2021,
   title={Comparison of Czech Transformers on¬†Text Classification Tasks},
   ISBN={9783030895792},
   ISSN={1611-3349},
   url={http://dx.doi.org/10.1007/978-3-030-89579-2_3},
   DOI={10.1007/978-3-030-89579-2_3},
   booktitle={Statistical Language and Speech Processing},
   publisher={Springer International Publishing},
   author={Leheƒçka, Jan and ≈†vec, Jan},
   year={2021},
   pages={27‚Äì37} }

@online{RobeCzech,
  author    = {Straka, Milan and N√°plava, Jakub and Strakov√°, Jana and Samuel, David},
  title     = {Model Card for RobeCzech},
  year      = {2024},
  url       = {https://huggingface.co/ufal/robeczech-base},
  urldate   = {2025-05-01},
}

@article{JMLR:v21:20-074,
  author  = {Colin Raffel and Noam Shazeer and Adam Roberts and Katherine Lee and Sharan Narang and Michael Matena and Yanqi Zhou and Wei Li and Peter J. Liu},
  title   = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer},
  journal = {Journal of Machine Learning Research},
  year    = {2020},
  volume  = {21},
  number  = {140},
  pages   = {1--67},
  url     = {http://jmlr.org/papers/v21/20-074.html}
}

@online{FERNET-C5,
  author    = {Leheƒçka, Jan and ≈†vec, Jan},
  title     = {FERNET-C5},
  year      = {2024},
  url       = {https://huggingface.co/fav-kky/FERNET-C5},
  urldate   = {2025-05-01},
}

@online{FERNET-C5-RoBERTa,
  author    = {Leheƒçka, Jan and ≈†vec, Jan},
  title     = {FERNET-C5-RoBERTa},
  year      = {2024},
  url       = {https://huggingface.co/fav-kky/FERNET-C5-RoBERTa},
  urldate   = {2025-05-01},
}

@online{Gemini,
  author    = {Google},
  title     = {Gemini 2.0 Flash},
  year      = {2025},
  url       = {https://ai.google.dev/gemini-api/docs/models#gemini-2.0-flash},
  urldate   = {2025-05-02},
}

@online{Joshparameters,
  author    = {Josh Howarth},
  title     = {Number of Parameters in GPT-4 (Latest Data)},
  year      = {2025},
  url       = {https://explodingtopics.com/blog/gpt-parameters},
  urldate   = {2025-05-04},
}

@online{ChatGPT,
  author    = {OpenAi},
  title     = {ChatGPT},
  year      = {2025},
  url       = {https://chatgpt.com},
  urldate   = {2025-05-04},
}

@online{Gemini2,
  author    = {Google},
  title     = {Google AI Studio},
  year      = {2025},
  url       = {https://aistudio.google.com/prompts/new_chat},
  urldate   = {2025-05-04},
}

@online{Qwen,
  author    = {Alibaba Cloud},
  title     = {Qwen},
  year      = {2025},
  url       = {https://chat.qwen.ai},
  urldate   = {2025-05-04},
}

@online{phi4,
  author    = {Microsoft},
  title     = {Phi-4},
  year      = {2025},
  url       = {https://huggingface.co/microsoft/phi-4},
  urldate   = {2025-05-04},
}

@online{Deepseek,
  author    = {Deepseek},
  title     = {Deepseek},
  year      = {2025},
  url       = {https://chat.deepseek.com},
  urldate   = {2025-05-04},
}

@online{Claude,
  author    = {Anthropic},
  title     = {Claude},
  year      = {2025},
  url       = {https://claude.ai/chat},
  urldate   = {2025-05-04},
}